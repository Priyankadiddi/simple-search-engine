{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "TaUzDhpzifVg",
    "outputId": "b42e6458-a672-48b3-c8f5-50d0d8572ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFB8ayhyimYq"
   },
   "outputs": [],
   "source": [
    "import os, time, re, random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "tiA7cAPViwmc",
    "outputId": "29420f4c-78b4-44e0-8f5c-cb43eff99c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q tensorflow_gpu>=2.0\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "ZKPACH36izTn",
    "outputId": "5ea4f150-e383-4109-dc44-4a5f209e333d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 102kB 2.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 6.7MB 10.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.0MB 43.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 245kB 57.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.8MB 42.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 481kB 70.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.1MB 40.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 870kB 53.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.0MB 24.5MB/s \n",
      "\u001b[?25h  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O0JhHKO9jqFs",
    "outputId": "82cde578-d375-4baf-886f-b0a0f62360b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JAQ_AdctnKI-"
   },
   "outputs": [],
   "source": [
    "d = \"/content/drive/My Drive/Colab Notebooks/Datasets/Personal Case Study/Search Engine/\"\n",
    "model_path = d+\"Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7-ZrlcmojGEE",
    "outputId": "b3fc9a71-6b3b-4315-c504-9ea3fb675c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    this will load the data\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(d+\"data.tsv\", sep = \"\\t\", header = None, low_memory = True)\n",
    "    col_names = [\"que_no\", \"question\", \"answer\", \"label\", \"ans_no\"]\n",
    "    print(\"Before number column in dataset are : \", df.shape[1])\n",
    "    df.columns = col_names\n",
    "    # Note: ans_no, que_no column is not useful for this probelm, so I'm droping this column\n",
    "    df.drop([\"que_no\",\"ans_no\"], axis = 1, inplace = True)\n",
    "    print(\"After droping number columns in dataset are: \", df.shape[1])\n",
    "    print(\"Number of datapoints in dataset : {:,}\".format(df.shape[0]))\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "#removing \"not\" stop word from stop_words\n",
    "stop_words = stop_words - {\"not\", \"what\", \"when\", \"where\", \"how\", \"which\"}\n",
    "\n",
    "##########################################################################################################\n",
    "#################################### function to pre-process the text ####################################\n",
    "##########################################################################################################\n",
    "def text_process(row):\n",
    "    \"\"\"\n",
    "    this function is for pre-processing the text.\n",
    "    exppand the sentences, remove stopwords, removes html, remove extra space etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = row\n",
    "        text = str(text).lower()\n",
    "        porter = PorterStemmer()\n",
    "\n",
    "        #expansion\n",
    "        text = text.replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"itis\")\\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar\")\\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "\n",
    "        text = re.sub(r\"<.*?>\",\"\", text) # removes the htmltags: https://stackoverflow.com/a/12982689\n",
    "\n",
    "        #special character removal\n",
    "        text = re.sub('[^a-zA-Z0-9\\n]', ' ', text)\n",
    "        #extra space removal\n",
    "        text = re.sub('\\s+',' ', text)\n",
    "\n",
    "        # stopword removal\n",
    "        text_to_words = []\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                text_to_words.append(word)\n",
    "            else:\n",
    "                continue\n",
    "        text = \" \".join(text_to_words)\n",
    "\n",
    "        # stemm the words of sentence\n",
    "        text = porter.stem(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    except:\n",
    "        print(\"There is no value in comment_text, so returnin 'nan'\")\n",
    "        \n",
    "        return np.nan\n",
    "\n",
    "##########################################################################################################\n",
    "#################################### function to design the features  ####################################\n",
    "##########################################################################################################\n",
    "\n",
    "def hand_craft_features(data_df):\n",
    "    \"\"\"\n",
    "    this creates the hand crafted features.\n",
    "    \"\"\"\n",
    "    # lenth features\n",
    "    data_df[\"que_len\"] = data_df[\"question\"].apply(lambda x:len(x))\n",
    "    data_df[\"ans_len\"] = data_df[\"answer\"].apply(lambda x:len(x))\n",
    "    data_df[\"que_word\"] = data_df[\"question\"].apply(lambda x:len(x.split()))\n",
    "    data_df[\"ans_word\"] = data_df[\"answer\"].apply(lambda x:len(x.split()))\n",
    "    data_df[\"unique_word_que\"] = data_df[\"question\"].apply(lambda x:len(set(x.split())))\n",
    "    data_df[\"unique_word_ans\"] = data_df[\"answer\"].apply(lambda x:len(set(x.split())))\n",
    "    #ratio features\n",
    "    data_df[\"ratio_q_a_len\"] = data_df[\"que_len\"]/data_df[\"ans_len\"]\n",
    "    data_df[\"ratio_q_a_word\"] = data_df[\"que_word\"]/data_df[\"ans_word\"]\n",
    "    data_df[\"ratio_q_a_unique_word\"] = data_df[\"unique_word_que\"]/data_df[\"unique_word_ans\"]\n",
    "    data_df[\"ration_qword_alen\"] = data_df[\"que_word\"]/data_df[\"ans_len\"]\n",
    "    data_df[\"ratio_aword_qlen\"] = data_df[\"ans_word\"]/data_df[\"que_len\"]\n",
    "    data_df[\"ratio_unique_qword_aword\"] = data_df[\"unique_word_que\"]/data_df[\"ans_word\"]\n",
    "    data_df[\"ratio_unique_aword_qword\"] = data_df[\"unique_word_ans\"]/data_df[\"que_word\"]\n",
    " \n",
    "    df.replace([np.inf], np.nan, inplace = True)\n",
    "    data_df.fillna(0, inplace=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "##########################################################################################################\n",
    "#################################### function to plot confudion matrix   #################################\n",
    "##########################################################################################################\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_C_P_R_matrix(y_true, y_pred, name):\n",
    "    \"\"\"\n",
    "    Funtion to plot confusion, precision and recall matrix\n",
    "    \"\"\"\n",
    "    C = confusion_matrix(y_true, y_pred) #for confusion matrix\n",
    "    # for presion matrix: take the sum column wise, i.e. column normalization\n",
    "    P = C/C.sum(axis = 0) # it'll have column sum = 1\n",
    "    #for recall matrix: take the sum row wise, i.e. row normalization\n",
    "    R = (C.T/C.sum(axis = 1)).T #it'll have row sum = 1\n",
    "\n",
    "    fig, (ax1, ax2, ax3)  = plt.subplots(1,3, figsize= (19,4))\n",
    "\n",
    "    #https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "    sns.heatmap(C, annot = True, fmt = 'd', annot_kws={\"size\":15}, ax = ax1)\n",
    "    ax1.set_title(\"Confusion Matrix\", fontsize = 15)\n",
    "    ax1.set_xlabel(\"Predicted\", fontsize = 12)\n",
    "    ax1.set_ylabel(\"Actual\", fontsize = 12)\n",
    "\n",
    "    sns.heatmap(P, annot = True, fmt = '.3f', annot_kws={\"size\":15}, ax = ax2)\n",
    "    ax2.set_title(\"Precision Matrix\", fontsize = 15)\n",
    "    ax2.set_xlabel(\"Predicted\", fontsize = 12)\n",
    "    ax2.set_ylabel(\"Actual\", fontsize = 12)\n",
    "\n",
    "    sns.heatmap(R, annot = True, fmt = '.3f', annot_kws={\"size\":15}, ax = ax3)\n",
    "    ax3.set_title(\"Recall Matrix\", fontsize = 15)\n",
    "    ax3.set_xlabel(\"Predicted\", fontsize = 12)\n",
    "    ax3.set_ylabel(\"Actual\", fontsize = 12)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "########################################################################################\n",
    "#######################    function to plot Confusion matrix     #######################\n",
    "########################################################################################\n",
    "def plot_confusion_matrix(y_train_pred, y_cv_pred, y_test_pred, y_train, y_cv, y_test, thres):\n",
    "    \"\"\"\n",
    "    funtion to plot the precison, recall and confusion matrix for train, cv and test data\n",
    "    \"\"\"\n",
    "    tr_pred = np.where(y_train_pred >= thres, 1, 0)\n",
    "    cv_pred = np.where(y_cv_pred >= thres, 1, 0)\n",
    "    te_pred = np.where(y_test_pred >= thres, 1, 0)   \n",
    "\n",
    "    print(\" \"*30, \"*\"*20, \"For Train data\", \"*\"*20)\n",
    "    plot_C_P_R_matrix(y_train.values.flatten(), tr_pred, name = \"Train\")\n",
    "\n",
    "    print(\" \"*30, \"*\"*20, \"For CV data\", \"*\"*20)\n",
    "    plot_C_P_R_matrix(y_cv.values.flatten(), cv_pred, name = \"CV\")\n",
    "\n",
    "    print(\" \"*30, \"*\"*20, \"For Test data\", \"*\"*20)\n",
    "    plot_C_P_R_matrix(y_test.values.flatten(), te_pred, name = \"Test\")\n",
    "\n",
    "########################################################################################################\n",
    "############################      function to calculate the MRR        #################################\n",
    "########################################################################################################\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "def get_mrr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    to calcuate the mrr.\n",
    "    it take the y_true and y_pred and make list of lists such that\n",
    "    for each question ten answer result in list.\n",
    "    \"\"\"\n",
    "    y_true = np.array([y_true[i*10:(i+1)*10] for i in range(len(y_true)//10)])\n",
    "    y_pred = np.array([y_pred[i*10:(i+1)*10] for i in range(len(y_pred)//10)])\n",
    "    return label_ranking_average_precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yei5WCL-jHJf"
   },
   "outputs": [],
   "source": [
    "def train(train_it = False):\n",
    "    if train_it:\n",
    "        if not os.path.isfile(d+\"Processed Data/text_processed.csv\"):\n",
    "            print(\"processed data is not in drive, let's create...\")\n",
    "            #laod the data\n",
    "            df = load_data()\n",
    "            # pre-process text data\n",
    "            df.loc[:,\"question\"] = df[\"question\"].apply(text_process)\n",
    "            df.loc[:,\"answer\"] = df[\"answer\"].apply(text_process)\n",
    "            # relacing such question and answers with np.Nan and then droping them\n",
    "            df.replace([\"\", \"null\"], np.nan, inplace = True)\n",
    "            df.dropna(axis = 0, how = \"any\", inplace= True)\n",
    "            # drop duplicates\n",
    "            df.drop_duplicates(subset = [\"question\", \"answer\"], keep = False, inplace = True)\n",
    "\n",
    "            # Let's remove the questions that have more than 1 correct answer\n",
    "            # https://stackoverflow.com/a/43501282/12005970\n",
    "            a = df.groupby('question')\\\n",
    "                    .agg({'answer': 'count', 'label': 'sum'})\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={'answer':'total answers', 'label': 'correct answers'})\n",
    "            # questions that have more than 1 correct answer\n",
    "            multiple_correct_answers_df = a[(a['correct answers']>1) \n",
    "                                            or (a['correct answers'] == 0)\n",
    "                                            or (a['total answers'] < 10)\n",
    "                                            or (a['total answers'] > 10)]\n",
    "            df.drop(df[df['question'].isin(multiple_correct_answers)].index, axis = 0, inplace=True)\n",
    "            df.reset_index(inplace=True, drop= True)\n",
    "\n",
    "            # design text based features\n",
    "            df = hand_craft_features(df)\n",
    "            df = df.head(500000)\n",
    "        else:\n",
    "            print(\"laoding the processed dataset from drive...\")\n",
    "            df = pd.read_csv(d+\"Processed Data/text_processed.csv\", low_memory=True)\n",
    "            df = df.head(500000)\n",
    "        \n",
    "        # split the dataset\n",
    "        print(\"splitting the data...\")\n",
    "        df_train = df.iloc[0:300000, :] \n",
    "        df_cv    = df.iloc[300000:400000, :]\n",
    "        df_test  = df.iloc[400000:, :]\n",
    "        \n",
    "        print(\"Converting text into vectors...\")\n",
    "        # definig the model name\n",
    "        MODEL_NAME = 'distilbert-base-uncased'\n",
    "        # getting the Transformer\n",
    "        t = text.Transformer(MODEL_NAME, maxlen = 200, classes=[0,1])\n",
    "        # pre-prcoessing the data for training the model \n",
    "        trn = t.preprocess_train(list(df_train['question'] + ['[SEP]'] + df_train['answer']),\n",
    "                                df_train['label'].values)\n",
    "        val = t.preprocess_test(list(df_cv['question'] + ['[SEP]'] + df_cv['answer']),\n",
    "                                df_cv['label'].values)\n",
    "        model = t.get_classifier()\n",
    "        # creating the batchs of data created in above lines\n",
    "        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=64)\n",
    "        # model trainig\n",
    "        learner.fit_onecycle(1*10**(-4), 2)\n",
    "        #save the predictor\n",
    "        predictor = ktrain.get_predictor(learner.model, preproc=t)\n",
    "        predictor.save(model_path+'bert/')\n",
    "    else:\n",
    "        print('model is already trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dyDRCG10oxEP",
    "outputId": "d164db49-b33a-4f6f-cf75-2f8e32f2f7df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is already trained\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbI28pe7mMF2"
   },
   "source": [
    "## **`final1()` fnction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKo8Ou-smcMT"
   },
   "source": [
    "* This function will take the data in (X,Y) format give the performance of model.\n",
    "* It is like evaluating function for the model.\n",
    "\n",
    "**Note:** Before sending (X,Y) in function make sure that they follow these conditions-\n",
    "* Each question must have only 10 answer and only one of them should be correct.\n",
    "* First 10 row will correspond first question, next 10 rows will correspond to next question, next 10 rows will correspond to third question and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WXEOKfhmZ8z"
   },
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "predictor = ktrain.load_predictor(model_path+'bert/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rjTr4Aujpl1p",
    "outputId": "65447628-8abb-4a9e-d477-bae12269b549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting the data...\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset. it satisfies above two condition\n",
    "df = pd.read_csv(d+\"Processed Data/text_processed.csv\", low_memory=True)\n",
    "df = df.head(500000)\n",
    "# split the dataset\n",
    "print(\"splitting the data...\")\n",
    "df_train = df.iloc[0:300000, :] \n",
    "df_cv    = df.iloc[300000:400000, :]\n",
    "df_test  = df.iloc[400000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USBxC1qIpZVB"
   },
   "outputs": [],
   "source": [
    "X = df_cv[['question', 'answer']]\n",
    "Y = df_cv['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YMGuAh7Hp5Hw"
   },
   "outputs": [],
   "source": [
    "def final1(X,Y, already_pred = True):\n",
    "    '''\n",
    "    this function will take (X,Y) and returns the MRR.\n",
    "    this alos print the confusion, precision and recall matrix.\n",
    "    X: dataframe: it has two columns [['question','answer']]\n",
    "    Y: it can dataframe, or panadas series. it has the class labels\n",
    "    already_pred: if predicted class-proabilities is stored in disk or not\n",
    "    X and Y must satisfy this two condition, other actual result be differe\n",
    "    1. each question must have only 10 answers and only of them should be correct.\n",
    "    2. First 10 row will correspond first question, next 10 rows will correspond to next question,\n",
    "     next 10 rows will correspond to third question and so on.\n",
    "    '''\n",
    "    if already_pred:\n",
    "        print('for given X model prediction is saved. So loading from the disk')\n",
    "        df = pd.read_csv(d+'cv_pred.csv')\n",
    "        mrr = get_mrr(df['actual_label'].values, df['class1_proba'].values)\n",
    "    else:\n",
    "        # prediction\n",
    "        Y_pred = predictor.predict_proba(list(X['question'] + ['[SEP]'] + X['answer']))[:,1]\n",
    "        # calculating the MRR\n",
    "        mrr = get_mrr(Y.values.flatten(), Y_pred.flatten())\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TX5v0_HjtmgQ",
    "outputId": "27643908-7637-4c1c-97d4-7c4fbc14d32c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for given X model prediction is saved. So loading from the disk\n",
      "MRR = 0.5607\n"
     ]
    }
   ],
   "source": [
    "mrr = final1(X,Y)\n",
    "print('MRR = {:.4f}'.format(mrr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6s_KAq_CmWOX"
   },
   "source": [
    "## **`final2()` function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5slLv8DRubE6"
   },
   "source": [
    "* This function will take the `X` only predict whether the question is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjtUP_zdfeGR"
   },
   "outputs": [],
   "source": [
    "def final2(X):\n",
    "    '''\n",
    "    this function take `X` and return predicted Y {0,1}\n",
    "    X: is dataframe: [['question','answer']]\n",
    "    function will predict the label of question and answer pair.\n",
    "\n",
    "    this function will return the class and class-probobabilities of each class\n",
    "    '''\n",
    "    #connvert text into sequence\n",
    "    print('text to sequnce...')\n",
    "    Y_pred = predictor.predict_proba(list(X['question'] + ['[SEP]'] + X['answer']))\n",
    "    label = np.where(Y_pred[:,1]>=0.2,1,0)\n",
    "    return label, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "fTeR8jlKpwIh",
    "outputId": "4a8c47ba-fea2-445f-84a0-a58555d012ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text to sequnce...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = unbreakable kimmy schmidt cast breaks season 2 finale shockers stars ellie kemper tituss burgess jane krakowski carol kane discuss season biggest moments preview what ahead season three everyone unbreakable kimmy schmidt ends season two different place where started\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 0\n"
     ]
    }
   ],
   "source": [
    "# as one datapoints\n",
    "X = df_cv[['question', 'answer', 'label']].head(1)\n",
    "label, Y_pred = final2(X)\n",
    "print(f'for Question = {X.question.values[0]}\\n\\\n",
    "    Answer = {X.answer.values[0]}\\n\\\n",
    "    Actual class label = {X.label.values[0]}\\n\\\n",
    "    Predicted class label = {label[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "0FtudAhKqNn0",
    "outputId": "35df9ad6-1e01-4143-c162-16c234847fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text to sequnce...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = unbreakable kimmy schmidt cast breaks season 2 finale shockers stars ellie kemper tituss burgess jane krakowski carol kane discuss season biggest moments preview what ahead season three everyone unbreakable kimmy schmidt ends season two different place where started\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 0\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = unbreakable kimmy schmidt unbreakable kimmy schmidt american web television sitcom created tina fey robert carlock starring ellie kemper title role streamed netflix since march 6 2015 originally set 13 episode first season nbc spring 2015 show sold netflix given two season order\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 1\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = hollywood memorable movies actually remakes films across globe check international films hits inspired\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 0\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = lisa lynn masters dies unbreakable kimmy schmidt actress 52 lisa lynn masters guest starred unbreakable kimmy schmidt also roles gossip girl ugly betty died traveling peru 52 rep confirmed death deadline details provided masters born omaha ne grew asheville nc\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 1\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = audience reviews unbreakable kimmy schmidt season 2 afraid coming season potential sophomore slump save first episode unbreakable kimmy schmidt funny ever new cast members cameo appearances add fun\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 0\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = tituss burgess favorite emoji tituss burgess plays titus andromedon unbreakable kimmy schmidt told huffpost favorite emoji praise hands when something good itis like hallelujah said impression emoji emoji wishes existed\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 0\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = unbreakable kimmy schmidt cast breaks season 2 finale shockers stars ellie kemper tituss burgess jane krakowski carol kane discuss season biggest moments preview what ahead season three warning story contains spoilers unbreakable kimmy schmidt entire second season\n",
      "    Actual class label = 1\n",
      "    Predicted class label = 1\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = cast unbreakable kimmy schmidt reveal favorite emojis emojis language internet when went webby awards monday night ask stars favorite emojis\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 0\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = rescued underground bunker which lived past 15 years kimmy schmidt decides move new york normal life makes friends new roommate titus works babysitter jacqueline voorhees wife billionaire many issues\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 0\n",
      "for Question = cast unbreakable kimmy schmidt\n",
      "    Answer = stars netflix comedy unbreakable kimmy schmidt told us which emojis favorites which ones felt missing feeling titular character would obsessed confounded emojis actor ellie kemper simply loves\n",
      "    Actual class label = 0\n",
      "    Predicted class label = 0\n"
     ]
    }
   ],
   "source": [
    "# for multiple inputs\n",
    "X = df_cv[['question', 'answer', 'label']].head(10)\n",
    "label, Y_pred = final2(X)\n",
    "for i in range(10):\n",
    "    print(f'for Question = {X.question.values[i]}\\n\\\n",
    "    Answer = {X.answer.values[i]}\\n\\\n",
    "    Actual class label = {X.label.values[i]}\\n\\\n",
    "    Predicted class label = {label[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "OyeIOUXFxHTE",
    "outputId": "d875d44c-30d1-4929-c425-da2d3cbba6f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91    270000\n",
      "           1       0.36      0.75      0.49     30000\n",
      "\n",
      "    accuracy                           0.84    300000\n",
      "   macro avg       0.67      0.80      0.70    300000\n",
      "weighted avg       0.91      0.84      0.87    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# for training data\n",
    "# loading saved the predicted values\n",
    "df = pd.read_csv(d+'train_pred.csv') \n",
    "print(classification_report(df['actual_label'], np.where(df['class1_proba'].values>=0.2,1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "CxzgNnb6yXmu",
    "outputId": "9a2104e4-66f3-43bd-e925-2add617849b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.87     90000\n",
      "           1       0.24      0.55      0.33     10000\n",
      "\n",
      "    accuracy                           0.78    100000\n",
      "   macro avg       0.59      0.68      0.60    100000\n",
      "weighted avg       0.87      0.78      0.81    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for CV data\n",
    "# loading saved the predicted values\n",
    "df = pd.read_csv(d+'cv_pred.csv') \n",
    "print(classification_report(df['actual_label'], np.where(df['class1_proba'].values>=0.2,1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "WOOlSbD-lChn",
    "outputId": "b140badb-b06b-4b9e-9775-0122557424f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89     90000\n",
      "           1       0.27      0.53      0.36     10000\n",
      "\n",
      "    accuracy                           0.81    100000\n",
      "   macro avg       0.61      0.69      0.63    100000\n",
      "weighted avg       0.87      0.81      0.84    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for test data\n",
    "# loading saved the predicted values\n",
    "df = pd.read_csv(d+'test_pred.csv') \n",
    "print(classification_report(df['actual_label'], np.where(df['class1_proba'].values>=0.2,1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGS9d3zHtfld"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Search Engine (final with BERT).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
